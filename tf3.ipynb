{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHHHHl6Cwn2mBglU8JXPvN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V4g2mZUIWP4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9up7u7H9JkFu"
      },
      "source": [
        "#IMDB(Internet Movie Database) 데이터셋 다운로드#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aCpiOVHIiwL",
        "outputId": "fda18578-8513-41e8-e002-df241e07dd67"
      },
      "source": [
        "imdb = keras.datasets.imdb\r\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\n",
        "# num_words = 10000은 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어\r\n",
        "# 단어의 시퀀스(리스트)는 전처리되어 정수형으로 변환되어 있다고 합니다."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENB6dlSxJo-d"
      },
      "source": [
        "#데이터 탐색#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aEfMWMxJae8",
        "outputId": "8d4ce804-869b-406f-e067-cc45a0bb1bd9"
      },
      "source": [
        "print(\"훈련 샘플: {}, 레이블: {}\".format(len(train_data), len(train_labels)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플: 25000, 레이블: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAmw2W6iNSsD",
        "outputId": "1672817b-1790-4eb8-f639-1f6d02333221"
      },
      "source": [
        "print(train_data[0])\r\n",
        "# 리뷰가 어휘 사전의 특정 단어를 나태내는 정수로 변환되어있음."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3veGlsL1NWmu",
        "outputId": "00ce794d-690c-4c66-c2e3-3e651c05095f"
      },
      "source": [
        "len(train_data[0]), len(train_data[1])\r\n",
        "# 영화의 리뷰들은 길이가 다르기 때문에 리뷰 데이터의 길이도 다름\r\n",
        "# 이후에 신경망 입력을 위해서 길이 차이 문제를 해결해야 함"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218, 189)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVYvOBjSXFmf",
        "outputId": "388238fc-81ee-48c2-8d82-767e12db6357"
      },
      "source": [
        " # 단어와 정수 인덱스를 매핑한 딕셔너리\r\n",
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59p4BvgkbYJr"
      },
      "source": [
        "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\r\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\r\n",
        "word_index[\"<PAD>\"] = 0\r\n",
        "word_index[\"<START>\"] = 1\r\n",
        "word_index[\"<UNK>\"] = 2  # unknown\r\n",
        "word_index[\"<UNUSED>\"] = 3\r\n",
        "\r\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\r\n",
        "# ('a',2) 이렇게 2개의 원소로 묶여있는 튜플이 리스트의 원소로 있다면 그 리스트는 딕셔너리형으로 변환 가능 -> dict()로 묶어서\r\n",
        "def decode_review(text):\r\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\r\n",
        "# get(찾을 값, 디폴트 값(찾는 게 없다면))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "s913762BbpBD",
        "outputId": "94bb8804-7bd4-492e-f570-c4c163e619e9"
      },
      "source": [
        "decode_review(train_data[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOCopmW8gx2r"
      },
      "source": [
        "#데이터 준비#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB8n5xbcjFxb"
      },
      "source": [
        "1. 원-핫 인코딩을 사용한다 -> 단어의 개수 * 리뷰의 수 크기의 행렬이 필요\r\n",
        "> [2. 예시](https://rekt77.tistory.com/102)\r\n",
        "2. 정수 배열의 길이가 모두 같도록 패딩을 추가하여 최대길이 * 리뷰의 수 크기의 정수 텐서를 생성함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF024xzQaJSD"
      },
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\r\n",
        "                                                        value=word_index[\"<PAD>\"],\r\n",
        "                                                        padding='post',\r\n",
        "                                                        maxlen=256)\r\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\r\n",
        "                                                       value=word_index[\"<PAD>\"],\r\n",
        "                                                       padding='post',\r\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkThx_r5u9-g",
        "outputId": "a7692a86-6279-4a03-ca57-38a2df9efade"
      },
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmc_RnLhvGrf",
        "outputId": "67cb3a03-8ec1-4f3c-d248-91410019fdd2"
      },
      "source": [
        "print(train_data[0])\r\n",
        "# 패딩 완료"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
            "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
            "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
            "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
            " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
            "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
            "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
            " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
            "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
            "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
            "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
            "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
            " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy3Su1OiwIGW"
      },
      "source": [
        "#모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1POG6xOvvKjH",
        "outputId": "84b4c1d1-57df-4c81-b692-4e083e346e32"
      },
      "source": [
        "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\r\n",
        "vocab_size = 10000\r\n",
        "\r\n",
        "model = keras.Sequential()\r\n",
        "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,))) # 256 개의 단어 당 크기 16의 임베딩 벡터를 만듦\r\n",
        "model.add(keras.layers.GlobalAveragePooling1D()) # 256 개의 단어에서 256 방향으로(sequence 차원) 평균을 계산하여 데이터의 형태가 (25000,16)으로 변함\r\n",
        "model.add(keras.layers.Dense(16, activation='relu'))\r\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}